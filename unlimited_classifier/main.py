from typing import Tuple, List, Union

from transformers import ( # type: ignore
    PreTrainedTokenizer,
    PreTrainedTokenizerFast,
    PreTrainedModel, 
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    GenerationMixin,
    TFGenerationMixin,
    FlaxGenerationMixin,
)
from transformers.utils import ModelOutput # type: ignore
import torch

from unlimited_classifier.trie import LabelsTrie

class TextClassifier:
    """
    Class for text classification
    """

    prompt: str = "Classify the following text: "
    model: PreTrainedModel

    def initialize_labels_trie(self, labels: List[str]) -> None:
        """
        Initializing the labels trie

        Args:
            labels (List[str]): Labels that will be used.
        """        
        self.trie = LabelsTrie([
            [0] + self.tokenizer.encode(label) # type: ignore
            for label in labels
        ])


    def __init__(
        self,
        labels: List[str], 
        model: Union[str, PreTrainedModel],
        tokenizer: Union[
            str, PreTrainedTokenizer, PreTrainedTokenizerFast
        ],
        device: str="cpu",
        num_beams: int=5,
        max_new_tokens: int=512,
    ) -> None:
        """
        Args:
            labels (List[str]): Labels for classification.
            
            model (Union[str, PreTrainedModel]): Model.
            
            tokenizer (Union[
                str, PreTrainedTokenizer, PreTrainedTokenizerFast
            ]): Tokenizer.
            
            device (str, optional): Device. Defaults to "cpu".
            
            num_beams (int, optional): Number of beams. Defaults to 5.

            max_new_tokens (int, optional): Maximum newly generated tokens.
        Defaults to 512.

        Raises:
            ValueError: If no labels are provided.

            ValueError: If a generative model is expected but not provided.
        """        
        if not labels:
            raise ValueError("No labels provided.")
        
        self.device = device
        self.num_beams = min(num_beams, len(labels))
        self.max_new_tokens = max_new_tokens

        if isinstance(model, str):
            try:
                self.model = ( # type: ignore
                    AutoModelForSeq2SeqLM # type: ignore
                    .from_pretrained(model)
                    .to(self.device)
                )
            except Exception:
                raise ValueError("Expected generative model.")
        else:
            self.model = model

        if not any(
            isinstance(self.model, t) for t in ( # type: ignore
                GenerationMixin, TFGenerationMixin, FlaxGenerationMixin
            ) 
        ):
            raise ValueError("Expected generative model.")
        
        if isinstance(tokenizer, str):
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer) # type: ignore
        else:
            self.tokenizer = tokenizer

        self.initialize_labels_trie(labels)


    def predict(self, prompts: List[str]) -> ModelOutput:
        """
        Model prediction

        Args:
            prompts (List[str]): Texts joined with prompts.

        Returns:
            ModelOutput: Output generated by the model.
        """        
        outputs = self.model.generate( # type: ignore
            **self.tokenizer( # type: ignore
                prompts, 
                return_tensors="pt",
                padding=True, 
                truncation=True
            ).to(self.device),
            max_new_tokens=512,
            num_beams=self.num_beams,
            num_return_sequences=self.num_beams,
            return_dict_in_generate=True, output_scores=True,
            prefix_allowed_tokens_fn=(
                lambda _, sent: self.trie.get(sent.tolist()) or [0] # type: ignore
            )
        )
        return outputs # type: ignore


    def invoke(self, text: str) -> List[Tuple[str, float]]:
        """
        Invokation for single text

        Args:
            text (str): Text for classification.

        Returns:
            List[Tuple[str, float]]: List of tuples containing classes and
        their corresponding scores. The tuples are sorted by score in 
        descending order.
        """        
        return self.invoke_batch([text])[0]


    def invoke_batch(self, texts: List[str]) -> List[List[Tuple[str, float]]]:
        """
        Invokation for multiple texts

        Args:
            texts (List[str]): Texts for classification.

        Returns:
            List[List[Tuple[str, float]]]: A list of lists where each inner
        list contains tuples of classes and their corresponding scores for the
        corresponding texts. The tuples are sorted by score in descending order.
        The order of the inner lists matches the order of the input texts.
        """        
        outputs = self.predict([self.prompt + text for text in texts])
        decodes = self.tokenizer.batch_decode( # type: ignore
            outputs.sequences, # type: ignore
            skip_special_tokens=True
        )

        if self.num_beams == 1:
            scores = torch.ones(len(decodes))
        else:
            scores = outputs.sequences_scores # type: ignore
        
        outputs2scores: List[List[Tuple[str, float]]] = []
        for text_id in range(len(texts)):
            batch: List[Tuple[str, float]] = []
            for beam_id in range(self.num_beams):
                score = scores[text_id*self.num_beams+beam_id] # type: ignore
                p = torch.exp(score).item() # type: ignore
                label = decodes[text_id*self.num_beams+beam_id]
                batch.append((label, p))
            outputs2scores.append(batch)
        return outputs2scores
    

    async def ainvoke(self, text: str) -> List[Tuple[str, float]]:
        """
        Async invoke

        Args:
            text (str): Text for classification.

        Returns:
            List[Tuple[str, float]]: List of tuples containing classes and
        their corresponding scores. The tuples are sorted by score in 
        descending order.
        """        
        return self.invoke(text)


    async def ainvoke_batch(self, texts: List[str]) -> List[List[Tuple[str, float]]]:
        """
        Async invokation for multiple texts

        Args:
            texts (List[str]): Texts for classification.

        Returns:
            List[List[Tuple[str, float]]]: A list of lists where each inner
        list contains tuples of classes and their corresponding scores for the
        corresponding texts. The tuples are sorted by score in descending order.
        The order of the inner lists matches the order of the input texts.
        """    
        return self.invoke_batch(texts)