from typing import Tuple, List, Union, Optional

from transformers import ( # type: ignore
    PreTrainedTokenizer,
    PreTrainedTokenizerFast,
    PreTrainedModel,
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    GenerationMixin,
    TFGenerationMixin,
    FlaxGenerationMixin,
    AutoConfig,
    AutoModelForCausalLM
)
from transformers.utils import ModelOutput # type: ignore
import torch
import pyximport # type: ignore
pyximport.install() # type: ignore

from unlimited_classifier.labels_trie import LabelsTrie # type: ignore
from unlimited_classifier.scorer import Scorer

class TextClassifier:
    """
    Class for text classification
    """

    prompt: str = "Classify the following text: "
    model: PreTrainedModel

    def initialize_labels_trie(self, labels: List[str]) -> None:
        """
        Initializing the labels trie

        Args:
            labels (List[str]): Labels that will be used.
        """        
        self.trie: LabelsTrie = LabelsTrie([ # type: ignore
            [self.pad_token] + self.tokenizer.encode(label) # type: ignore
            for label in labels
        ])


    def initiaize_model(self, model: str) -> PreTrainedModel:
        try:
            return ( # type: ignore
                AutoModelForSeq2SeqLM # type: ignore
                .from_pretrained(model)
                .to(self.device)
            )
        except Exception:
            pass
        
        try:
            config = AutoConfig.from_pretrained('bert-base-cased') # type: ignore
            return AutoModelForCausalLM.from_config(config) # type: ignore
        except Exception:
            raise ValueError("Expected generative model.")


    def __init__(
        self,
        labels: List[str], 
        model: Union[str, PreTrainedModel],
        tokenizer: Union[
            str, PreTrainedTokenizer, PreTrainedTokenizerFast
        ],
        device: str="cpu",
        num_beams: int=5,
        max_new_tokens: int=512,
        pad_token: Optional[int]=None,
        scorer: Optional[Scorer]=None,
    ) -> None:
        """
        Args:
            labels (List[str]): Labels for classification.
            
            model (Union[str, PreTrainedModel]): Model.
            
            tokenizer (Union[
                str, PreTrainedTokenizer, PreTrainedTokenizerFast
            ]): Tokenizer.
            
            device (str, optional): Device. Defaults to "cpu".
            
            num_beams (int, optional): Number of beams. Defaults to 5.

            max_new_tokens (int, optional): Maximum newly generated tokens.
        Defaults to 512.

        Raises:
            ValueError: If no labels are provided.

            ValueError: If a generative model is expected but not provided.
        """        
        if not labels:
            raise ValueError("No labels provided.")
        
        self.device = device
        self.num_beams = min(num_beams, len(labels))
        self.max_new_tokens = max_new_tokens
        self.scorer = scorer

        if isinstance(model, str):
            self.model = self.initiaize_model(model)
        else:
            self.model = model

        if (
            not any(
                isinstance(self.model, t) for t in ( # type: ignore
                    GenerationMixin, 
                    TFGenerationMixin, 
                    FlaxGenerationMixin
                ) 
            ) 
            and not self.model.config.is_decoder # type: ignore
            and not self.model.config.is_encoder_decoder # type: ignore
        ): 
            raise ValueError("Expected generative model.")
        
        if isinstance(tokenizer, str):
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer) # type: ignore
        else:
            self.tokenizer = tokenizer

        if self.tokenizer.pad_token is None: # type: ignore
            tokenizer.add_special_tokens( # type: ignore
                {"pad_token": "[PAD]"}, 
                replace_additional_special_tokens=False
            )
            model.resize_token_embeddings(len(tokenizer)) # type: ignore

        self.tokenizer.padding_side = "left" # type: ignore

        self.pad_token = (
            pad_token
            if not pad_token is None
            else self.tokenizer.pad_token_id
        )
        self.initialize_labels_trie(labels)


    def predict(self, prompts: List[str]) -> ModelOutput:
        """
        Model prediction

        Args:
            prompts (List[str]): Texts joined with prompts.

        Returns:
            ModelOutput: Output generated by the model.
        """        
        outputs = self.model.generate( # type: ignore
            **self.tokenizer( # type: ignore
                prompts, 
                return_tensors="pt",
                padding=True,
                truncation=True
            ).to(self.device),
            pad_token_id=self.pad_token,
            max_new_tokens=self.max_new_tokens,
            num_beams=self.num_beams,
            num_return_sequences=self.num_beams,
            return_dict_in_generate=True,
            output_scores=True,
            prefix_allowed_tokens_fn=(
                lambda _, sent: 
                self.trie.get(sent.tolist()) # type: ignore
                or [self.tokenizer.eos_token_id] 
            )
        )
        return outputs # type: ignore


    def invoke(self, text: str) -> List[Tuple[str, float]]:
        """
        Invokation for single text

        Args:
            text (str): Text for classification.

        Returns:
            List[Tuple[str, float]]: List of tuples containing classes and
        their corresponding scores. The tuples are sorted by score in 
        descending order.
        """        
        return self.invoke_batch([text])[0]


    def invoke_batch(self, texts: List[str]) -> List[List[Tuple[str, float]]]:
        """
        Invokation for multiple texts

        Args:
            texts (List[str]): Texts for classification.

        Returns:
            List[List[Tuple[str, float]]]: A list of lists where each inner
        list contains tuples of classes and their corresponding scores for the
        corresponding texts. The tuples are sorted by score in descending order.
        The order of the inner lists matches the order of the input texts.
        """        
        outputs = self.predict([self.prompt + text for text in texts])
        decodes = self.tokenizer.batch_decode( # type: ignore
            outputs.sequences, # type: ignore
            skip_special_tokens=True
        )

        if self.num_beams == 1:
            scores = torch.ones(len(decodes))
        else:
            scores = outputs.sequences_scores # type: ignore
        
        outputs2scores: List[List[Tuple[str, float]]] = []
        for text_id in range(len(texts)):
            batch: List[Tuple[str, float]] = []
            for beam_id in range(self.num_beams):
                score = scores[text_id*self.num_beams+beam_id] # type: ignore
                p = torch.exp(score).item() # type: ignore
                label = decodes[text_id*self.num_beams+beam_id]
                batch.append((label, p))
            outputs2scores.append(
                self.scorer.score(texts[text_id], batch) 
                if self.scorer 
                else batch
            )
        return outputs2scores
    

    async def ainvoke(self, text: str) -> List[Tuple[str, float]]:
        """
        Async invoke

        Args:
            text (str): Text for classification.

        Returns:
            List[Tuple[str, float]]: List of tuples containing classes and
        their corresponding scores. The tuples are sorted by score in 
        descending order.
        """        
        return self.invoke(text)


    async def ainvoke_batch(self, texts: List[str]) -> List[List[Tuple[str, float]]]:
        """
        Async invokation for multiple texts

        Args:
            texts (List[str]): Texts for classification.

        Returns:
            List[List[Tuple[str, float]]]: A list of lists where each inner
        list contains tuples of classes and their corresponding scores for the
        corresponding texts. The tuples are sorted by score in descending order.
        The order of the inner lists matches the order of the input texts.
        """    
        return self.invoke_batch(texts)