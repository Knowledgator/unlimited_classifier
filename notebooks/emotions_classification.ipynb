{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unlimited-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unlimited_classifier import TextClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion\n",
    "emotion_dataset = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "n = 1000\n",
    "test_dataset = emotion_dataset['test'].select(range(n))\n",
    "\n",
    "\n",
    "classes = test_dataset.features[\"label\"].names\n",
    "\n",
    "idx2class = {idx:class_ for idx, class_ in enumerate(classes)}\n",
    "\n",
    "N=8\n",
    "train_dataset = emotion_dataset['train'].shuffle(seed=41).select(range(len(classes)*N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_name = 'google/flan-t5-base'\n",
    "model_name = 'knowledgator/flan-t5-large-for-classification'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "You can run model in zero-shot setting as well as additionally fine-tune on few-examples or providing the in a prompt.\n",
    "\n",
    "**Skip this if you don't want to fine-tune the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "results = []\n",
    "for id in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    examples = test_dataset[id:id+batch_size]\n",
    "    texts = examples['text']\n",
    "    output = classifier.invoke_batch(texts)\n",
    "    predicts = [res[0] for res in output]\n",
    "    results+=predicts\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prefix = prompt\n",
    "\n",
    "    inputs = [prefix + str(doc) for doc in examples[\"text\"]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "\n",
    "        labels = tokenizer([idx2class[id] for id in examples[\"label\"]], max_length=8, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    del examples['label']\n",
    "    del examples['text']\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/classifier_t5\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    save_steps = 300,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the following text and return just the single emotion name that represents it.\n",
    "\n",
    "Text:\"\"\"\n",
    "\n",
    "classifier = TextClassifier(\n",
    "    labels=classes,\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    device='cuda:0',\n",
    "    num_beams=1,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "results = []\n",
    "for id in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    examples = test_dataset[id:id+batch_size]\n",
    "    output = classifier.invoke_batch(texts)\n",
    "    predicts = [res[0] for res in output]\n",
    "    results+=predicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class2idx = {class_:idx for idx, class_ in idx2class.items()}\n",
    "predicts = [class2idx[res[0]] for res in results]\n",
    "labels = test_dataset['label']\n",
    "print(classification_report(labels, predicts, target_names=classes, digits=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
